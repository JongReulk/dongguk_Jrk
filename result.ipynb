{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from konlpy.tag import Kkma #pip install jpype1 #pip install konlpy\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "from collections import Counter #워드 클라우드에서 단어들을 쉽게 집계하기 위해 사용\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #pip install scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from ole import OleFile #pip install ole-py\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "#from gensim.models.doc2vec import Doc2Vec\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "wikimodel_dir='C:/doc2vec/docmodel/w2v.model'\n",
    "wikimodel_dir2='C:/doc2vec/model/wiki.model'\n",
    "\n",
    "dgu_dir='C:/doc2vec/test/Dgu1.txt'\n",
    "query_dir='C:/doc2vec/query/query.txt'\n",
    "\n",
    "\n",
    "model = Word2Vec.load(wikimodel_dir2)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "import pytagcloud\n",
    "import simplejson\n",
    "import sys\n",
    "\n",
    "import openpyxl # excel파일 읽기\n",
    "\n",
    "import olefile # 워드파일읽기\n",
    "\n",
    "import matplotlib.pyplot as plt #워드클라우드\n",
    "\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, process_pdf #pip install pdfminer3k\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "from io import open #pdf 파일 읽기위함\n",
    "\n",
    "stopwords = ['번호', '작성자', '작성', '대학', '회수','안내', '공지', '게시','내용','보기','보내기','사항', '학생', '기간', '첨부파일', '학기','신청','학점','학년','지원',\n",
    "             '국제','방법','확인','선발','지원','모집','교류','실습', '첨부파일','다운로드', '결과']\n",
    "\n",
    "stopwords2 = []\n",
    "\n",
    "\n",
    "#i=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['척추동물', '돼지', '수퇘지', '멧돼지', '주둥이', '페커리', '동물', '사파리', '가축']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dict_m=[]\n",
    "labels= ['척추동물', '돼지', '수퇘지', '멧돼지', '주둥이', '페커리', '동물', '사파리', '가축']\n",
    "print(labels)\n",
    "\n",
    "class SentenceTokenizer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.twitter = Twitter()\n",
    "        self.kkma = Kkma()\n",
    "        \n",
    "        \n",
    "    def text2sentences(self, text):\n",
    "        sentences22 = self.kkma.sentences(text)\n",
    "        for idx in range(0, len(sentences22)):\n",
    "            if len(sentences22[idx]) <= 10:\n",
    "                sentences22[idx-1] += (' ' + sentences22[idx])\n",
    "                sentences22[idx] = ''\n",
    "\n",
    "        return sentences22\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_nouns(self, sentences):\n",
    "\n",
    "        nouns = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "\n",
    "            if sentence is not '':\n",
    "\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence))\n",
    "\n",
    "                                       if noun not in stopwords and len(noun) > 1]))\n",
    "\n",
    "        return nouns\n",
    "\n",
    "\n",
    "class GraphMatrix(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "\n",
    "        self.graph_sentence = []\n",
    "\n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        \n",
    "        #print('문장 가공전2222',tfidf_mat)\n",
    "        #print('크기2222',tfidf_mat.shape)\n",
    "\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "\n",
    "        return self.graph_sentence\n",
    "    \n",
    "    #labels= ['척추동물', '돼지', '수퇘지', '멧돼지', '주둥이', '페커리', '동물', '사파리', '가축']\n",
    "    \n",
    "    def build_words_graph(self, sentence):\n",
    "        \n",
    "        cnt_vec_mat2=self.cnt_vec.fit_transform(sentence).toarray().astype(float)\n",
    "        \n",
    "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        \n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        \n",
    "        sorted_vocab_idx = sorted(vocab.items())\n",
    "        \n",
    "        print('sorted_vocab_idx', sorted_vocab_idx)\n",
    "        \n",
    "        print(len(sorted_vocab_idx))\n",
    "        \n",
    "        for d,c in sorted_vocab_idx :\n",
    "            dict_m.append(d)\n",
    "        \n",
    "        for i in labels :\n",
    "            cnt_col=0\n",
    "            for j in dict_m:\n",
    "                #print('j',j)\n",
    "                cnt_col+=1\n",
    "                model_sim = model.wv.similarity(i,j)\n",
    "                cnt_vec_mat2[:,(cnt_col-1):cnt_col]+=model_sim\n",
    "\n",
    "        cnt_vec_mat99 = normalize(cnt_vec_mat2.astype(float), axis=0)\n",
    "        \n",
    "        total_cnt=np.dot(cnt_vec_mat99.T, cnt_vec_mat99)\n",
    "        \n",
    "        total_origin=np.dot(cnt_vec_mat.T, cnt_vec_mat)\n",
    "        \n",
    "        print(total_cnt)\n",
    "        \n",
    "        print(total_origin)\n",
    "        \n",
    "        \n",
    "\n",
    "        return total_origin, {vocab[word]: word for word in vocab}\n",
    "\n",
    "\n",
    "class Rank(object):\n",
    "\n",
    "    def get_ranks(self, graph, d=0.85):  # d = damping factor\n",
    "\n",
    "        A = graph\n",
    "\n",
    "        matrix_size = A.shape[0]\n",
    "\n",
    "        for id in range(matrix_size):\n",
    "\n",
    "            A[id, id] = 0  # diagonal 부분을 0으로\n",
    "\n",
    "            link_sum = np.sum(A[:, id])  # A[:, id] = A[:][id]\n",
    "\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "\n",
    "            A[:, id] *= -d\n",
    "\n",
    "            A[id, id] = 1\n",
    "\n",
    "        #print('A\\n',A)\n",
    "        B = (1 - d) * np.ones((matrix_size, 1))\n",
    "        #print('B\\n',B)\n",
    "\n",
    "        ranks = np.linalg.solve(A, B)  # 연립방정식 Ax = b\n",
    "\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "\n",
    "class TextRank(object):\n",
    "\n",
    "    def __init__(self, text):\n",
    "\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        \n",
    "        #self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "\n",
    "        self.sentences=text\n",
    "        #print('\\n문장 개수',len(self.sentences))\n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "\n",
    "        #print('nouns\\n',nouns)\n",
    "\n",
    "        self.graph_matrix = GraphMatrix()\n",
    "\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        \n",
    "        #print(''self.sent_graph)\n",
    "\n",
    "        #print('self.sent_graph\\n',self.sent_graph)\n",
    "        #print(self.sent_graph.shape)\n",
    "\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "\n",
    "        print('words_graph\\n',self.words_graph)\n",
    "        print(self.words_graph.shape)\n",
    "\n",
    "        self.rank = Rank()\n",
    "\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        \n",
    "        #print('self.sent_rank_idx',self.sent_rank_idx)\n",
    "        \n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        \n",
    "        #print('self.sorted_sent_rank_idx\\n\\n',self.sorted_sent_rank_idx)\n",
    "\n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        \n",
    "        print('self.word_rank_idx',self.word_rank_idx)\n",
    "\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "        \n",
    "        print('self.sorted_word_rank_idx',self.sorted_word_rank_idx)\n",
    "\n",
    "    def summarize(self, sent_num=5):\n",
    "\n",
    "        summary = []\n",
    "\n",
    "        index = []\n",
    "\n",
    "        if len(self.sentences) > 30 :\n",
    "            sent_num=7\n",
    "\n",
    "        elif len(self.sentences) > 100:\n",
    "            sent_num=10\n",
    "\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "\n",
    "        index.sort()\n",
    "\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def keywords(self, word_num=10):\n",
    "\n",
    "        rank = Rank()\n",
    "\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "\n",
    "        keywords = []\n",
    "\n",
    "        index = []\n",
    "\n",
    "        #if len(self.sentences) > 30 :\n",
    "            #word_num=15\n",
    "\n",
    "        #elif len(self.sentences) > 100:\n",
    "            #word_num=20\n",
    "\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "\n",
    "        # index.sort()\n",
    "\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "\n",
    "        return keywords\n",
    "\n",
    "def read_pdf_file(pdfFile):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "\n",
    "    process_pdf(rsrcmgr, device, pdfFile)\n",
    "    device.close()\n",
    "\n",
    "    content = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "오리지날 C:/doc2vec/result/pre_word/test10.txt\n",
      "결과 total:\n",
      " ['\\ufeff강원도 철원의 야생 멧돼지 폐사체 등에서 아프리카돼지열병(ASF) 2건이 추가로 확인됐다. \\n', 'ASF 검출 사례는 2건이 추가되면서 전국에서 41건으로 늘었다.\\n', '환경부 소속 국립환경과학원은 지난 4일 오후 10시 30분쯤 민통선 남쪽 3㎞ 지점인 철원군 서면에서 총기로 포획한 멧돼지와 5일 오전 11시쯤 민통선 남쪽 13㎞ 철원군 갈말읍에서 발견된 멧돼지 폐사체에서 ASF 바이러스가 각각 검출됐다고 7일 밝혔다. \\n', '이에 따라 멧돼지에서 ASF바이러스가 검출된 사례는 전국에서 41건, 철원에서 15건으로 늘었다.\\n', '야생 멧돼지가 발견된 철원 갈마읍에는 10㎞이내에 46개 농가(철원군 31, 포천 15개)가 있다. \\n', '이들 농가에서는 돼지 약 10만5000마리를 사육 중이다. \\n', '멧돼지를 총기로 포획한 철원군 서면은 발견지점에서 10㎞이내에 12개의 농가가 있다. \\n', '이곳에선 약 4만1000마리의 돼지를 사육하고 있다.\\n', '농식품부는 ASF 바이러스 검출이 확인된 직후 반경 10㎞ 내 농가와 경기, 강원 전체 양돈 농가에 문자메시지를 전송해 이 사실을 알렸다. \\n', '또 농장 내부 소독과 울타리 방역 시설을 점검했다. \\n', '철원군과 포천시에는 양성 개체 발견 지점 10㎞ 내 농가에 이동 제한 조치를 내리기도 했다. \\n', '농식품부는 이날 철원군과 인접 지역인 화천군, 포천시에 직원들을 파견해 지역 내 농가 울타리를 점검하고 방역 조치가 제대로 이뤄졌는지 확인할 계획이다.']\n",
      "12\n",
      "sorted_vocab_idx [('각각', 0), ('갈말읍', 1), ('강원', 2), ('강원도', 3), ('개체', 4), ('경기', 5), ('계획', 6), ('국립', 7), ('남쪽', 8), ('내부', 9), ('농가', 10), ('농식품부', 11), ('농장', 12), ('돼지', 13), ('리기', 14), ('마리', 15), ('멧돼지', 16), ('문자메시지', 17), ('민통선', 18), ('바이러스', 19), ('반경', 20), ('발견', 21), ('방역', 22), ('사례', 23), ('사실', 24), ('사육', 25), ('사체', 26), ('서면', 27), ('소독', 28), ('소속', 29), ('시설', 30), ('아프리카', 31), ('야생', 32), ('양돈', 33), ('양성', 34), ('열병', 35), ('오전', 36), ('오후', 37), ('울타리', 38), ('이내', 39), ('이동', 40), ('인접', 41), ('전국', 42), ('전송', 43), ('전체', 44), ('점검', 45), ('제대로', 46), ('제한', 47), ('조치', 48), ('중이', 49), ('지난', 50), ('지역', 51), ('지점', 52), ('직원', 53), ('직후', 54), ('철원', 55), ('철원군', 56), ('총기', 57), ('추가', 58), ('파견', 59), ('포천', 60), ('포천시', 61), ('포획', 62), ('화천군', 63), ('환경과학', 64), ('환경부', 65)]\n",
      "66\n",
      "[[ 1.          0.99961609 -0.89831098 ...  0.88819085  0.98931386\n",
      "   0.9991609 ]\n",
      " [ 0.99961609  1.         -0.90700268 ...  0.89829117  0.99297375\n",
      "   0.99991211]\n",
      " [-0.89831098 -0.90700268  1.         ... -0.92851927 -0.93626456\n",
      "  -0.91091539]\n",
      " ...\n",
      " [ 0.88819085  0.89829117 -0.92851927 ...  1.          0.93364471\n",
      "   0.90288029]\n",
      " [ 0.98931386  0.99297375 -0.93626456 ...  0.93364471  1.\n",
      "   0.99445536]\n",
      " [ 0.9991609   0.99991211 -0.91091539 ...  0.90288029  0.99445536\n",
      "   1.        ]]\n",
      "[[1. 1. 0. ... 0. 1. 1.]\n",
      " [1. 1. 0. ... 0. 1. 1.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 1. 0. ... 0. 1. 1.]\n",
      " [1. 1. 0. ... 0. 1. 1.]]\n",
      "words_graph\n",
      " [[1. 1. 0. ... 0. 1. 1.]\n",
      " [1. 1. 0. ... 0. 1. 1.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 1. 0. ... 0. 1. 1.]\n",
      " [1. 1. 0. ... 0. 1. 1.]]\n",
      "(66, 66)\n",
      "self.word_rank_idx {0: 1.2094733976565037, 1: 1.2094733976565042, 2: 0.9481764955437476, 3: 0.7645259985154885, 4: 0.7379130636111791, 5: 0.9481764955437475, 6: 1.0266035471073878, 7: 1.2094733976565042, 8: 1.2094733976565042, 9: 0.6583541092512405, 10: 1.646806424510514, 11: 1.3711888009878521, 12: 0.6583541092512405, 13: 0.9521962415803868, 14: 0.7379130636111789, 15: 0.5996882727300366, 16: 1.529010673436176, 17: 0.9481764955437472, 18: 1.209473397656504, 19: 1.4322405279299457, 20: 0.9481764955437475, 21: 1.3714856706551404, 22: 1.177922222037901, 23: 0.454639439552795, 24: 0.9481764955437479, 25: 0.5996882727300362, 26: 1.3857183194055323, 27: 1.13220102405571, 28: 0.6583541092512406, 29: 1.2094733976565044, 30: 0.6583541092512405, 31: 0.7645259985154884, 32: 0.8622318196907793, 33: 0.9481764955437475, 34: 0.7379130636111794, 35: 0.7645259985154886, 36: 1.209473397656504, 37: 1.2094733976565037, 38: 1.177922222037901, 39: 0.6147054801723365, 40: 0.7379130636111794, 41: 1.026603547107388, 42: 0.454639439552795, 43: 0.9481764955437477, 44: 0.9481764955437475, 45: 1.177922222037901, 46: 1.0266035471073878, 47: 0.737913063611179, 48: 1.2250348523731693, 49: 0.5175090011392731, 50: 1.209473397656504, 51: 1.026603547107388, 52: 1.3271695598085342, 53: 1.026603547107388, 54: 0.9481764955437475, 55: 0.9083337378036458, 56: 1.7312256118163547, 57: 1.1322010240557103, 58: 0.7548897946238243, 59: 1.026603547107388, 60: 0.4656130543725615, 61: 1.2250348523731693, 62: 1.1322010240557099, 63: 1.0266035471073878, 64: 1.2094733976565037, 65: 1.2094733976565037}\n",
      "self.sorted_word_rank_idx [56, 10, 16, 19, 26, 21, 11, 52, 48, 61, 29, 1, 7, 8, 18, 36, 50, 0, 37, 64, 65, 22, 38, 45, 57, 27, 62, 41, 51, 53, 59, 6, 46, 63, 13, 24, 43, 2, 5, 20, 33, 44, 54, 17, 55, 32, 35, 3, 31, 58, 34, 40, 4, 47, 14, 28, 9, 12, 30, 39, 15, 25, 49, 60, 23, 42]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kang\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n",
      "C:\\Users\\kang\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "path_dir='C:/doc2vec/word_pre/txt/'\n",
    "path_dir2='C:/doc2vec/word_pre/txt2/'\n",
    "path_dir3='C:/doc2vec/word_pre/txt3/'\n",
    "path_dir4='C:/doc2vec/word_pre/txt4/'\n",
    "path_dir5='C:/doc2vec/word_pre/txt5/'\n",
    "path_dir6='C:/doc2vec/word_pre/txt6/'\n",
    "\n",
    "jpg_dir='C:/doc2vec/word_pre/picture/'\n",
    "pdf_dir='C:/doc2vec/word_pre/pdf/'\n",
    "hwp_dir='C:/doc2vec/word_pre/hwp/'\n",
    "\n",
    "path_pre='C:/doc2vec/result/pre_word/'\n",
    "summary_dir='C:/doc2vec/word_result/summary_test/'\n",
    "cloud_dir = 'C:/doc2vec/word_pre/cloud/'\n",
    "keyword_before='C:/doc2vec/result/keyword_before/'\n",
    "full_dir='C:/doc2vec/word_result/total_test/'\n",
    "keyword_after='C:/doc2vec/result/keyword_after/'\n",
    "\n",
    "\n",
    "txt_list=os.listdir(path_dir) #폴더안에 있는 파일 리스트로 저장\n",
    "txt_list2=os.listdir(path_dir2)\n",
    "txt_list3=os.listdir(path_dir3)\n",
    "txt_list4=os.listdir(path_dir4)\n",
    "txt_list5=os.listdir(path_dir5)\n",
    "txt_list6=os.listdir(path_dir6)\n",
    "\n",
    "txt_pre=os.listdir(path_pre)\n",
    "str1=''\n",
    "total_word=[]\n",
    "total_word2=[]\n",
    "t = Twitter()\n",
    "#dir=path_dir\n",
    "sys.stdout.flush()\n",
    "\n",
    "for change in txt_pre:\n",
    "    total = []  # 통합문서\n",
    "\n",
    "    remover= change.replace(\".txt\",\"-\")\n",
    "    original=path_pre+change\n",
    "    etc_pdf= pdf_dir+ remover + '0.pdf'\n",
    "    etc_hwp = hwp_dir + remover + '0.hwp'\n",
    "    jpg_name = change.replace(\"txt\", \"jpg\")\n",
    "    jpg_file=jpg_dir+jpg_name\n",
    "    png_name = change.replace(\"txt\", \"png\")\n",
    "    png_file = jpg_dir + png_name\n",
    "    pdf_name = change.replace(\"txt\", \"pdf\")\n",
    "    pdf_file = pdf_dir + pdf_name\n",
    "    hwp_name = change.replace(\"txt\", \"hwp\")\n",
    "    hwp_file = hwp_dir + hwp_name\n",
    "    textocr=\"\"\n",
    "\n",
    "    \n",
    "    print('\\n오리지날',original)\n",
    "\n",
    "    input_file = open(original, \"r\", encoding=\"utf-8\")\n",
    "    text1 = input_file.readlines() #문서 불러오고 한줄씩 읽기\n",
    "    total += text1\n",
    "    input_file.close()\n",
    "    \n",
    "    if(os.path.isfile(jpg_file)==True ) :\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "        file_name = os.path.join(\n",
    "            os.path.dirname(__file__),\n",
    "            jpg_file)\n",
    "\n",
    "        with io.open(file_name, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "\n",
    "        image = vision.types.Image(content=content)\n",
    "\n",
    "        response = client.text_detection(image=image)\n",
    "        texts = response.text_annotations\n",
    "        textocr = texts[0].description\n",
    "\n",
    "        jpg1 = \"\"\n",
    "        jpg_list = []  # 이미지 리스트\n",
    "        for x in textocr:\n",
    "            if (x == '\\n'):\n",
    "                jpg_list.append(jpg1 + '\\n')\n",
    "                jpg1 = \"\"\n",
    "            else:\n",
    "                jpg1 = jpg1 + x\n",
    "        print('jpg 있음')\n",
    "        print()\n",
    "        total += jpg_list\n",
    "\n",
    "    #print('글 내용',total)\n",
    "\n",
    "\n",
    "\n",
    "    #total = text1 + jpg_list\n",
    "    #print('\\n전체 내용',total)\n",
    "    \n",
    "\n",
    "    final = [] # 통합문 리스트\n",
    "    for word in total:\n",
    "\n",
    "        if (len(word) == 1 or len(word) == 2):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            final.append(word)\n",
    "    # 불필요한 \\n 없애기위함\n",
    "\n",
    "    print('결과 total:\\n', final)\n",
    "    print(len(final))\n",
    "\n",
    "    total_word=t.nouns(str(final))\n",
    "    total_word2+=total_word\n",
    "\n",
    "\n",
    "    #print('원래텍스트 타입', (type(text1)))\n",
    "    #print('text1:', text1)\n",
    "\n",
    "    full_name=full_dir+change\n",
    "    full = open(full_name, 'w', encoding=\"utf-8\")\n",
    "    full.writelines(final)  # 리스트 형태인 sum2를 저장하기위해 씀\n",
    "\n",
    "    full.close()\n",
    "\n",
    "\n",
    "    textrank = TextRank(final)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sum2=textrank.summarize() # 요약문 함수\n",
    "    keyword_1=textrank.keywords()\n",
    "    #newsum=str(sum2)\n",
    "\n",
    "    temp=summary_dir+change\n",
    "\n",
    "    f = open(temp, 'w',encoding=\"utf-8\")\n",
    "    f.writelines(sum2) #리스트 형태인 sum2를 저장하기위해 씀\n",
    "    f.close()\n",
    "\n",
    "    keyword_temp=keyword_after+change\n",
    "    keyword_save = open(keyword_temp, 'w', encoding='utf-8')\n",
    "    for i in keyword_1 :\n",
    "        keyword_save.writelines(i)\n",
    "        keyword_save.writelines(\" \")\n",
    "\n",
    "    #keyword_save.writelines(keyword_1)\n",
    "    keyword_save.close()\n",
    "    \n",
    "    \n",
    "    #파일 저장\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
